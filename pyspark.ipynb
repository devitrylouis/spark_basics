{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark (with PySpark)\n",
    "\n",
    "\n",
    "In this tutorial, you will learn how to use Apache Spark, a framework for large-scale data processing, within a notebook.\n",
    "Upon completing this lab you will be able to : \n",
    " + Program in Spark with the Python Language\n",
    " + Demonstrate how to read and process data using Spark\n",
    " + Compare and contrast RDD and Dataframes. \n",
    " + Build a simple machine learning application with Spark.\n",
    " \n",
    " \n",
    " \n",
    "# Requirements : Installing Spark, PySpark and configuration to run it on jupyter notebooks\n",
    "\n",
    "According to your OS, you have to follow the following tutorial :\n",
    "\n",
    "+ [Windows](https://changhsinlee.com/install-pyspark-windows-jupyter/)\n",
    "+ [Mac Os X](https://jmedium.com/pyspark-in-mac/)\n",
    " \n",
    "At the end of the installation, you should be able to run the following code that is a kind of Hello word in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/Users/louisdevitry/Downloads/spark-2.4.0-bin-hadoop2.7')\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Row, functions\n",
    "from pyspark.ml.feature import CountVectorizer, StringIndexer\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|hello|\n",
      "+-----+\n",
      "|spark|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hello world\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = spark.sql('''select 'spark' as hello ''')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 :  Spark Context\n",
    "\n",
    "When writing a spark program, the first thing to do is to define a `SparkContext`. \n",
    "\n",
    "In Spark, communication occurs between a driver and executors. The driver has Spark jobs that it needs to run and these jobs are split into tasks that are submitted to the executors for completion. The results from these tasks are delivered back to the driver.\n",
    "\n",
    "Here, we will use the `findspark` package that has to be installed using the following command: \n",
    "\n",
    "` pip3 install findspark`\n",
    "\n",
    "Then we can use the `findspark.init()`function to locate the Spark process.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try printing out sc to see its type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use Python's `dir()` function to get a list of all the attributes (including methods) accessible through the `sc` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PACKAGE_EXTENSIONS',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__getattribute__',\n",
       " '__getnewargs__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__module__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_accumulatorServer',\n",
       " '_active_spark_context',\n",
       " '_batchSize',\n",
       " '_callsite',\n",
       " '_checkpointFile',\n",
       " '_conf',\n",
       " '_dictToJavaMap',\n",
       " '_do_init',\n",
       " '_encryption_enabled',\n",
       " '_ensure_initialized',\n",
       " '_gateway',\n",
       " '_getJavaStorageLevel',\n",
       " '_initialize_context',\n",
       " '_javaAccumulator',\n",
       " '_jsc',\n",
       " '_jvm',\n",
       " '_lock',\n",
       " '_next_accum_id',\n",
       " '_pickled_broadcast_vars',\n",
       " '_python_includes',\n",
       " '_repr_html_',\n",
       " '_serialize_to_jvm',\n",
       " '_temp_dir',\n",
       " '_unbatched_serializer',\n",
       " 'accumulator',\n",
       " 'addFile',\n",
       " 'addPyFile',\n",
       " 'appName',\n",
       " 'applicationId',\n",
       " 'binaryFiles',\n",
       " 'binaryRecords',\n",
       " 'broadcast',\n",
       " 'cancelAllJobs',\n",
       " 'cancelJobGroup',\n",
       " 'defaultMinPartitions',\n",
       " 'defaultParallelism',\n",
       " 'dump_profiles',\n",
       " 'emptyRDD',\n",
       " 'environment',\n",
       " 'getConf',\n",
       " 'getLocalProperty',\n",
       " 'getOrCreate',\n",
       " 'hadoopFile',\n",
       " 'hadoopRDD',\n",
       " 'master',\n",
       " 'newAPIHadoopFile',\n",
       " 'newAPIHadoopRDD',\n",
       " 'parallelize',\n",
       " 'pickleFile',\n",
       " 'profiler_collector',\n",
       " 'pythonExec',\n",
       " 'pythonVer',\n",
       " 'range',\n",
       " 'runJob',\n",
       " 'sequenceFile',\n",
       " 'serializer',\n",
       " 'setCheckpointDir',\n",
       " 'setJobDescription',\n",
       " 'setJobGroup',\n",
       " 'setLocalProperty',\n",
       " 'setLogLevel',\n",
       " 'setSystemProperty',\n",
       " 'show_profiles',\n",
       " 'sparkHome',\n",
       " 'sparkUser',\n",
       " 'startTime',\n",
       " 'statusTracker',\n",
       " 'stop',\n",
       " 'textFile',\n",
       " 'uiWebUrl',\n",
       " 'union',\n",
       " 'version',\n",
       " 'wholeTextFiles']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 : Understanding Spark RDDâ€™s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordCount in Spark\n",
    "In this part, we will write the wordcount in Spark and apply it on the novel Dracula of Bram Stocker (from the Gutemberg project)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing(text):\n",
    "    words = text.strip().split()\n",
    "    words_list = []\n",
    "    for word in words:\n",
    "        if word != ',':\n",
    "            words_list.append(word.lower())\n",
    "    return(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type\n",
      "<class 'pyspark.rdd.RDD'>\n",
      "\n",
      "\n",
      "'Display data'\n",
      "[u'The Project Gutenberg EBook of Dracula, by Bram Stoker',\n",
      " u'',\n",
      " u'This eBook is for the use of anyone anywhere at no cost and with',\n",
      " u'almost no restrictions whatsoever.  You may copy it, give it away or',\n",
      " u're-use it under the terms of the Project Gutenberg License included',\n",
      " u'with this eBook or online at www.gutenberg.org/license',\n",
      " u'',\n",
      " u'',\n",
      " u'Title: Dracula',\n",
      " u'']\n",
      "\n",
      "\n",
      "'Execute the command with the action `take()` and retrieve the first 10 words from the `flatMap()`transformation'\n",
      "[u'the',\n",
      " u'project',\n",
      " u'gutenberg',\n",
      " u'ebook',\n",
      " u'of',\n",
      " u'dracula,',\n",
      " u'by',\n",
      " u'bram',\n",
      " u'stoker',\n",
      " u'this']\n",
      "\n",
      "\n",
      "'Alphabetically sorted count:'\n",
      "[(u'\"\\'my', 2),\n",
      " (u'\"_17', 3),\n",
      " (u'\"_2', 2),\n",
      " (u'\"_24', 2),\n",
      " (u'\"_25', 4),\n",
      " (u'\"_6', 2),\n",
      " (u'\"_czarina', 2),\n",
      " (u'\"a', 12),\n",
      " (u'\"ah', 2),\n",
      " (u'\"ah,', 16)]\n",
      "\n",
      "\n",
      "'Frequency sorted count:'\n",
      "[(u'the', 7984),\n",
      " (u'and', 5754),\n",
      " (u'to', 4504),\n",
      " (u'i', 4499),\n",
      " (u'of', 3710),\n",
      " (u'a', 2933),\n",
      " (u'he', 2509),\n",
      " (u'in', 2475),\n",
      " (u'that', 2365),\n",
      " (u'was', 1804)]\n"
     ]
    }
   ],
   "source": [
    "# Initiate spark context\n",
    "sc=pyspark.SparkContext()\n",
    "\n",
    "# First read the [pg345.txt](./SparkData/pg345.txt) file.\n",
    "print('Data type')\n",
    "data = sc.textFile('./SparkData/pg345.txt')\n",
    "print(type(data))\n",
    "print('\\n')\n",
    "\n",
    "# To see the content, of the file, we need to run the action `collect` on the  RDD `data`\n",
    "# I only print 10 for simplification purposes. collect() has no further difficulty\n",
    "pp.pprint('Display data')\n",
    "pp.pprint(data.take(10))\n",
    "print('\\n')\n",
    "\n",
    "# With the RDD `data`, from the previous cell, execute a `flatMap()` for each line in the input and then convert it to lower case, remove the commas, split the words on a space and store in the RDD `words`\n",
    "data_cleaned = data.flatMap(processing)\n",
    "\n",
    "pp.pprint('Execute the command with the action `take()` and retrieve the first 10 words from the `flatMap()`transformation')\n",
    "pp.pprint(data_cleaned.take(10))\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# Perform a classic `map()` to create a tuple where each word has a count of 1\n",
    "counts = data_cleaned.map(lambda word: (word, 1))\n",
    "\n",
    "# Write the reducing function\n",
    "reduced_counts = counts.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Get all words that occur more than once\n",
    "filtered_counts = reduced_counts.filter(lambda x: x[1] > 1)\n",
    "\n",
    "# Sort them alphabetically\n",
    "sorted_elem = filtered_counts.sortBy(lambda x: x[0], ascending=True)\n",
    "pp.pprint('Alphabetically sorted count:')\n",
    "pp.pprint(sorted_elem.take(10))\n",
    "print('\\n')\n",
    "\n",
    "# Sort by appearance frequency\n",
    "pp.pprint('Frequency sorted count:')\n",
    "freq_counts = filtered_counts.sortBy(lambda x: x[1], ascending = False)\n",
    "pp.pprint(freq_counts.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple exercice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Create a Python collection of 10,000 integers'\n",
      "\n",
      "\n",
      "'Create a Spark base RDD from that collection'\n",
      "\n",
      "\n",
      "'Subtract one from each value using map'\n",
      "[80, 94, 74, 57, 98, 77, 58, 56, 56, 31]\n",
      "\n",
      "\n",
      "'Wordcount: Map and reduce by key'\n",
      "[(0, 93),\n",
      " (4, 93),\n",
      " (8, 115),\n",
      " (12, 89),\n",
      " (16, 96),\n",
      " (20, 114),\n",
      " (24, 96),\n",
      " (28, 87),\n",
      " (32, 106),\n",
      " (36, 121)]\n",
      "\n",
      "\n",
      "'Perform action count'\n",
      "[(85, 130),\n",
      " (87, 122),\n",
      " (36, 121),\n",
      " (34, 120),\n",
      " (54, 120),\n",
      " (89, 117),\n",
      " (55, 116),\n",
      " (8, 115),\n",
      " (72, 115),\n",
      " (97, 115)]\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(\"Create a Python collection of 10,000 integers\")\n",
    "integers = list(np.random.randint(low = 0, high = 100, size = 10000))\n",
    "print('\\n')\n",
    "\n",
    "pp.pprint('Create a Spark base RDD from that collection')\n",
    "sc=pyspark.SparkContext()\n",
    "data = sc.parallelize(integers)\n",
    "print('\\n')\n",
    "\n",
    "pp.pprint(\"Subtract one from each value using map\")\n",
    "data_minus = data.map(lambda x: x-1)\n",
    "pp.pprint(data_minus.take(10))\n",
    "print('\\n')\n",
    "\n",
    "pp.pprint(\"Wordcount: Map and reduce by key\")\n",
    "data_minus_map = data_minus.map(lambda word: (word, 1))\n",
    "data_minus_reduced = data_minus_map.reduceByKey(lambda a, b: a + b)\n",
    "pp.pprint(data_minus_reduced.take(10))\n",
    "print('\\n')\n",
    "\n",
    "pp.pprint(\"Perform action count\")\n",
    "sorted_elem = data_minus_reduced.sortBy(lambda x: x[1], ascending=False)\n",
    "pp.pprint(sorted_elem.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An improved WordCount\n",
    "\n",
    "Print the top 10 most frequent words with their probability of appearance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def regex_(string):\n",
    "    return(re.sub('[^A-Za-z0-9]+', '', string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Basic preprocessing'\n",
      "[u'the',\n",
      " u'project',\n",
      " u'gutenberg',\n",
      " u'ebook',\n",
      " u'of',\n",
      " u'dracula',\n",
      " u'by',\n",
      " u'bram',\n",
      " u'stoker',\n",
      " u'this']\n",
      "\n",
      "\n",
      "'Sort them alphabetically'\n",
      "[(u'', 703),\n",
      " (u'1', 19),\n",
      " (u'10', 6),\n",
      " (u'1030', 2),\n",
      " (u'11', 7),\n",
      " (u'12', 6),\n",
      " (u'13', 3),\n",
      " (u'14', 3),\n",
      " (u'15', 3),\n",
      " (u'16', 5)]\n",
      "\n",
      "\n",
      "'Sort by appearance frequency'\n",
      "[(u'the', 8037),\n",
      " (u'and', 5896),\n",
      " (u'i', 4712),\n",
      " (u'to', 4540),\n",
      " (u'of', 3738),\n",
      " (u'a', 2961),\n",
      " (u'in', 2558),\n",
      " (u'he', 2543),\n",
      " (u'that', 2455),\n",
      " (u'it', 2141)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instanciate spark context\n",
    "sc=pyspark.SparkContext()\n",
    "\n",
    "data = sc.textFile('./SparkData/pg345.txt')\n",
    "\n",
    "pp.pprint('Basic preprocessing')\n",
    "data_cleaned = data.flatMap(processing) # Transformation\n",
    "data_cleaned = data_cleaned.map(regex_) # Transformation\n",
    "pp.pprint(data_cleaned.take(10)) # Action\n",
    "print('\\n')\n",
    "\n",
    "# Format words\n",
    "counts = data_cleaned.map(lambda word: (word, 1)) # Transformation\n",
    "\n",
    "# Reduce by key\n",
    "reduced_counts = counts.reduceByKey(lambda a, b: a + b) # Transformation\n",
    "\n",
    "# Remove low occurence words\n",
    "filtered_counts = reduced_counts.filter(lambda x: x[1] > 1) # Transformation\n",
    "\n",
    "pp.pprint('Sort them alphabetically')\n",
    "sorted_elem = filtered_counts.sortBy(lambda x: x[0], ascending=True) # Transformation\n",
    "pp.pprint(sorted_elem.take(10)) # Action\n",
    "print('\\n')\n",
    "\n",
    "pp.pprint('Sort by appearance frequency')\n",
    "freq_counts = filtered_counts.sortBy(lambda x: x[1], ascending = False) # Transformation\n",
    "pp.pprint(freq_counts.take(10))# Action\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many times are the transformations evaluated? (Hint: it depends)\n",
    "One of spark RDDs feature is its lazy evaluation. It means that until an action is specified, there is no evaluation. Consequently, the number of evaluations depend on the number of actions we specified. However, we can reduce the number of evaluations using of the persist method so that each action results will be saved for the next actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 : Spark SQL and dataframes\n",
    "\n",
    "In this part, you will explore Spark DataFrames and the SQL Context. In particular, we will work on a database that contains a sample of the world population by working on data that comes from [pplapi](http://pplapi.com/). The file [agents.json](./SparkData/agents.json) is a file that was extracted from this api using the following command :\n",
    "\n",
    "\n",
    "`wget https://s3-eu-west-1.amazonaws.com/course.oc-static.com/courses/4297166/agents.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read.json(\"./SparkData/agents.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------------------+-------------------+------+\n",
      "|        country_name|        id|          latitude|          longitude|   sex|\n",
      "+--------------------+----------+------------------+-------------------+------+\n",
      "|               China| 227417393| 33.15219798270325| 100.85840672174572|  Male|\n",
      "|               Haiti|6821129477|19.325567983697297| -72.43795260265814|Female|\n",
      "|               India|2078667700|23.645271492037235|  80.85636526088884|Female|\n",
      "|               China| 477556555| 33.45864668881662|  93.33604038078953|Female|\n",
      "|               India|1379059984|28.816938290678692|   80.7728698035823|Female|\n",
      "|               India|2278934249|24.223974351280358|  80.14372690674512|  Male|\n",
      "|         Philippines|4380736204|12.409991630883784| 122.75874146810197|Female|\n",
      "|               India|1375733494|22.385712662257426|  77.90320433636231|Female|\n",
      "|             Nigeria|3693807307| 9.967458870426357|  7.562942449523648|Female|\n",
      "|                Mali|6552202234|16.882575147323337| -3.949079041016242|Female|\n",
      "|            Thailand|5057542320|13.501463223632582| 103.05981834176352|  Male|\n",
      "|               India|1806935868|  18.4459850484637|  74.28181188756791|  Male|\n",
      "|               China| 265404548|29.447948266668902| 106.56441719305467|Female|\n",
      "|               China|1272365006|35.450817753169304| 106.05970256662503|  Male|\n",
      "|            Pakistan|3427393483|29.930865126652122|  70.15711512834791|Female|\n",
      "|Central African R...|7029557128|  7.01006931921262| 21.005933006443826|Female|\n",
      "|               Haiti|6826219119|18.940384961447766| -72.32688350214578|  Male|\n",
      "|            Tanzania|5357138381|-5.983046108539422|  34.80683011234468|  Male|\n",
      "|               Ghana|6186906042| 7.969120948577613|-1.9820137477590403|Female|\n",
      "|              Brazil|3275490483|-9.425790844213656| -55.88198203024498|  Male|\n",
      "+--------------------+----------+------------------+-------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Spark instructions that enable to display the number of French agents.'\n",
      "+------------+----------+--------------------+--------------------+------+\n",
      "|country_name|        id|            latitude|           longitude|   sex|\n",
      "+------------+----------+--------------------+--------------------+------+\n",
      "|      France|5130782577|-0.21142875508479517|-0.00395021443374...|Female|\n",
      "|      France|5125653041|  1.5099359591520582| -1.7155442515387973|Female|\n",
      "|      France|5092935162| 0.06978158062530335|  -1.529365900793559|Female|\n",
      "|      France|5108968681|-0.15326107452236482|  2.1243709186708934|Female|\n",
      "|      France|5078973934|-0.06137848013048675| -1.4476884573473048|Female|\n",
      "+------------+----------+--------------------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pp.pprint('Spark instructions that enable to display the number of French agents.')\n",
    "df[df.country_name.isin([\"France\"])].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Spark instructions that enable to display the number of Indian female agents.'\n",
      "+------------+----------+------------------+-----------------+------+\n",
      "|country_name|        id|          latitude|        longitude|   sex|\n",
      "+------------+----------+------------------+-----------------+------+\n",
      "|       India|2078667700|23.645271492037235|80.85636526088884|Female|\n",
      "|       India|1379059984|28.816938290678692| 80.7728698035823|Female|\n",
      "|       India|1375733494|22.385712662257426|77.90320433636231|Female|\n",
      "|       India|1584815794|19.102700698004416|79.67843973466778|Female|\n",
      "|       India|1514105680|17.825246219527756|73.22156078852429|Female|\n",
      "+------------+----------+------------------+-----------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pp.pprint('Spark instructions that enable to display the number of Indian female agents.')\n",
    "df[df.country_name.isin([\"India\"]) & df.sex.isin([\"Female\"])].show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the notion of temporary view (function `createTempView`), create a temporary view associated to the dataframe `df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createTempView(\"temp_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write some SQL query on the resulting table as shown in the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+\n",
      "|     country_name|        id|\n",
      "+-----------------+----------+\n",
      "| French Polynesia|7170821229|\n",
      "|       Cabo Verde|7167692449|\n",
      "|         Suriname|7166451460|\n",
      "|         Suriname|7166235088|\n",
      "|            Macau|7166034642|\n",
      "|       Montenegro|7164357515|\n",
      "|Equatorial Guinea|7163867872|\n",
      "|           Bhutan|7163256789|\n",
      "|           Bhutan|7163004645|\n",
      "|           Bhutan|7162877973|\n",
      "+-----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT country_name,id FROM temp_table ORDER BY id DESC LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+--------------------+--------------------+------+\n",
      "|country_name|        id|            latitude|           longitude|   sex|\n",
      "+------------+----------+--------------------+--------------------+------+\n",
      "|      France|5130782577|-0.21142875508479517|-0.00395021443374...|Female|\n",
      "|      France|5125653041|  1.5099359591520582| -1.7155442515387973|Female|\n",
      "|      France|5092935162| 0.06978158062530335|  -1.529365900793559|Female|\n",
      "|      France|5108968681|-0.15326107452236482|  2.1243709186708934|Female|\n",
      "|      France|5078973934|-0.06137848013048675| -1.4476884573473048|Female|\n",
      "|      France|5132969816|  1.1828646583062592| -0.6655754887318799|Female|\n",
      "|      France|5079112248|  -0.217260169262005| 0.25087488920444284|Female|\n",
      "|      France|5108994896| 0.49047553633530405| 0.12140982654262467|Female|\n",
      "|      France|5130956195| -0.4702673586856353|  0.9266846158973026|  Male|\n",
      "|      France|5135909518|  1.0264852068813861| -0.2140207779096717|  Male|\n",
      "|      France|5126986401| -0.3000128566536097| 0.38070406466183515|  Male|\n",
      "|      France|5118238997|  0.9474034213864879|  1.1594401098214056|  Male|\n",
      "|      France|5078427269|  1.2383110079391852| -0.8057894807391559|Female|\n",
      "|      France|5103360620|  0.8804203939951878| -1.9584312089442544|Female|\n",
      "|      France|5087376103| 0.36735236174643593|-0.08389139935919032|  Male|\n",
      "|      France|5127306923|  0.3357881974469305| 0.07190625070884477|  Male|\n",
      "|      France|5105659966| 0.25437723190280187| -0.2670049016944363|Female|\n",
      "|      France|5080236246| -0.9793883947051585| 0.09417020992108467|Female|\n",
      "|      France|5098071394|-0.00716727104203...|  -0.564196422329451|Female|\n",
      "|      France|5135065608|  1.2161218788026498|  0.5966144433039097|Female|\n",
      "+------------+----------+--------------------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM temp_table WHERE country_name == 'France'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+------------------+-----------------+------+\n",
      "|country_name|        id|          latitude|        longitude|   sex|\n",
      "+------------+----------+------------------+-----------------+------+\n",
      "|       India|2078667700|23.645271492037235|80.85636526088884|Female|\n",
      "|       India|1379059984|28.816938290678692| 80.7728698035823|Female|\n",
      "|       India|1375733494|22.385712662257426|77.90320433636231|Female|\n",
      "|       India|1584815794|19.102700698004416|79.67843973466778|Female|\n",
      "|       India|1514105680|17.825246219527756|73.22156078852429|Female|\n",
      "|       India|2274764948| 19.82309181729447|71.97339986366181|Female|\n",
      "|       India|2019200843| 19.85322450012048|81.48236565311277|Female|\n",
      "|       India|1900697806| 25.00490234918041|73.99415281960854|Female|\n",
      "|       India|1504105198|19.179362533369744|75.62931818391733|Female|\n",
      "|       India|2188686500|17.218351214433053|80.27969374143316|Female|\n",
      "|       India|1686639083|24.359614315713024|79.39650850071789|Female|\n",
      "|       India|1550347907|26.574720440836586|74.47164134180663|Female|\n",
      "|       India|2290224796|24.320185611561925|74.87525955099318|Female|\n",
      "|       India|1362119225| 25.02892140472612|77.66914365096424|Female|\n",
      "|       India|2550898312|16.850913331839383|76.71796956164341|Female|\n",
      "|       India|2487620564| 19.10977229820752| 79.8288777764338|Female|\n",
      "|       India|1625564020| 22.21614927778045|73.39614326639496|Female|\n",
      "|       India|1472936119| 23.26312832799611|81.27077564653098|Female|\n",
      "|       India|2029196289|21.045638534744956|79.19602403557826|Female|\n",
      "|       India|2428920190|27.741586798700045|75.69923606624118|Female|\n",
      "+------------+----------+------------------+-----------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM temp_table WHERE country_name == 'India' AND sex == 'Female'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create a dataframe from an existing RDD as shown on the example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=pyspark.SparkContext()\n",
    "sqlContext = pyspark.SQLContext(sc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_rdd = sc.parallelize([\n",
    "        [1, 'cats are cute', 0],\n",
    "        [2, 'dogs are playfull', 0],\n",
    "        [3, 'lions are big', 1],\n",
    "        [4, 'cars are fast', 1]])\n",
    "users_rdd = sc.parallelize([\n",
    "        [0, 'Alice', 20],\n",
    "        [1, 'Bob', 23],\n",
    "        [2, 'Charles', 32]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- doc_id: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documents_df = documents_rdd.toDF(['doc_id', 'text', 'user_id'])\n",
    "users_df = users_rdd.toDF(['user_id', 'name', 'age'])\n",
    "\n",
    "# printing the inferred schema for documents\n",
    "documents_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some functions can be apply to a column or to different columns. Here, we compute the age avarage in the dataframe `users_df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|avg(age)|\n",
      "+--------+\n",
      "|    25.0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as fn\n",
    "\n",
    "user_age_df = users_df.select(fn.avg('age')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Compute the max of age in the dataframe `users_df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|max(age)|\n",
      "+--------+\n",
      "|      32|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TO DO \n",
    "user_age_df = users_df.select(fn.max('age')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join (as in SQL) the two dataframes `users_df` and `documents_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO \n",
    "inner_join = users_df.join(documents_df, users_df.user_id == documents_df.user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+---+------+-----------------+-------+\n",
      "|user_id| name|age|doc_id|             text|user_id|\n",
      "+-------+-----+---+------+-----------------+-------+\n",
      "|      0|Alice| 20|     1|    cats are cute|      0|\n",
      "|      0|Alice| 20|     2|dogs are playfull|      0|\n",
      "|      1|  Bob| 23|     3|    lions are big|      1|\n",
      "|      1|  Bob| 23|     4|    cars are fast|      1|\n",
      "+-------+-----+---+------+-----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inner_join.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outer Join (Left) (as in SQL) the two dataframes `users_df` and `documents_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO\n",
    "inner_join = users_df.join(documents_df, users_df.user_id == documents_df.user_id, 'left_outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---+------+-----------------+-------+\n",
      "|user_id|   name|age|doc_id|             text|user_id|\n",
      "+-------+-------+---+------+-----------------+-------+\n",
      "|      0|  Alice| 20|     1|    cats are cute|      0|\n",
      "|      0|  Alice| 20|     2|dogs are playfull|      0|\n",
      "|      1|    Bob| 23|     3|    lions are big|      1|\n",
      "|      1|    Bob| 23|     4|    cars are fast|      1|\n",
      "|      2|Charles| 32|  null|             null|   null|\n",
      "+-------+-------+---+------+-----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inner_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 : MLlib - Spark for data analysis and machine learning\n",
    "\n",
    "In this part, we will see how to create a text classifiation application with Spark. We will use some data from the newsgroups [Usenet](https://en.wikipedia.org/wiki/Usenet_newsgroup) and the objective is to predict the topic of the news.\n",
    " \n",
    "The train data is [here](./SparkData/20ng-train-all-terms.txt) and the test data [here](./SparkData/20ng-test-all-terms.txt).\n",
    "The are obtained using the following commands: \n",
    "`wget http://ana.cachopo.org/datasets-for-single-label-text-categorization/20ng-train-all-terms.txt`\n",
    "and\n",
    "`wget http://ana.cachopo.org/datasets-for-single-label-text-categorization/20ng-test-all-terms.txt`\n",
    "\n",
    "Load these data as some RDDs and translate them in DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|        _c0|                 _c1|\n",
      "+-----------+--------------------+\n",
      "|alt.atheism|alt atheism faq a...|\n",
      "|alt.atheism|alt atheism faq i...|\n",
      "|alt.atheism|re gospel dating ...|\n",
      "|alt.atheism|re university vio...|\n",
      "|alt.atheism|re soc motss et a...|\n",
      "|alt.atheism|re a visit from t...|\n",
      "|alt.atheism|re political athe...|\n",
      "|alt.atheism|re an anecdote ab...|\n",
      "|alt.atheism|re political athe...|\n",
      "|alt.atheism|re pompous ass km...|\n",
      "|alt.atheism|re pompous ass li...|\n",
      "|alt.atheism|re keith schneide...|\n",
      "|alt.atheism|re keith schneide...|\n",
      "|alt.atheism|re political athe...|\n",
      "|alt.atheism|re political athe...|\n",
      "|alt.atheism|re political athe...|\n",
      "|alt.atheism|re don t more inn...|\n",
      "|alt.atheism|re ancient islami...|\n",
      "|alt.atheism|re political athe...|\n",
      "|alt.atheism|re there must be ...|\n",
      "+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------+--------------------+\n",
      "|        _c0|                 _c1|\n",
      "+-----------+--------------------+\n",
      "|alt.atheism|re about the bibl...|\n",
      "|alt.atheism|re amusing atheis...|\n",
      "|alt.atheism|re yet more rushd...|\n",
      "|alt.atheism|re christian mora...|\n",
      "|alt.atheism|re after years ca...|\n",
      "|alt.atheism|re amusing atheis...|\n",
      "|alt.atheism|southern baptist ...|\n",
      "|alt.atheism|re amusing atheis...|\n",
      "|alt.atheism|re requests in ar...|\n",
      "|alt.atheism|re thoughts on ch...|\n",
      "|alt.atheism|re what s a shit ...|\n",
      "|alt.atheism|re christian mora...|\n",
      "|alt.atheism|re theism and fan...|\n",
      "|alt.atheism|re you will all g...|\n",
      "|alt.atheism|re you will all g...|\n",
      "|alt.atheism|re what s a shit ...|\n",
      "|alt.atheism|re societally acc...|\n",
      "|alt.atheism|re societally acc...|\n",
      "|alt.atheism|re christian mora...|\n",
      "|alt.atheism|re gulf war and p...|\n",
      "+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc=pyspark.SparkContext()\n",
    "sqlContext = pyspark.SQLContext(sc)\n",
    "\n",
    "def load_dataframe(path):\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    df = spark.read.csv(path, sep = \"\\t\", inferSchema=True, header = False)\n",
    "    return(df)\n",
    "\n",
    "train_data = load_dataframe(\"./SparkData/train-all-terms.txt\")\n",
    "test_data = load_dataframe(\"./SparkData/test-all-terms.txt\")\n",
    "\n",
    "train_data.show()\n",
    "test_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split _c1 columns in lists of words\n",
    "train_data = train_data.withColumn('_c1', functions.split(train_data._c1, ' '))\n",
    "test_data = test_data.withColumn('_c1', functions.split(test_data._c1, ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first step is to represent our data, i.e. the messages in the form of a bag-of-word representation using the spark method `CountVectorizer` documented [here](https://spark.apache.org/docs/2.1.0/ml-features.html#countvectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO  - bag of word representation of test and train data\n",
    "# fit a CountVectorizerModel from the corpus.\n",
    "cv = CountVectorizer(inputCol=\"_c1\", outputCol = 'features', vocabSize=1000)\n",
    "\n",
    "model = cv.fit(train_data)\n",
    "\n",
    "train_trans = model.transform(train_data)\n",
    "test_trans = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displayong of the distint labels in the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|_c0                     |\n",
      "+------------------------+\n",
      "|alt.atheism             |\n",
      "|comp.graphics           |\n",
      "|comp.os.ms-windows.misc |\n",
      "|comp.sys.ibm.pc.hardware|\n",
      "|comp.sys.mac.hardware   |\n",
      "|comp.windows.x          |\n",
      "|misc.forsale            |\n",
      "|rec.autos               |\n",
      "|rec.motorcycles         |\n",
      "|rec.sport.baseball      |\n",
      "|rec.sport.hockey        |\n",
      "|sci.crypt               |\n",
      "|sci.electronics         |\n",
      "|sci.med                 |\n",
      "|sci.space               |\n",
      "|soc.religion.christian  |\n",
      "|talk.politics.guns      |\n",
      "|talk.politics.mideast   |\n",
      "|talk.politics.misc      |\n",
      "|talk.religion.misc      |\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_trans.select(\"_c0\").distinct().sort(\"_c0\").show(truncate=False)#.sort(\"label\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now apply a [NaiveBayes](https://spark.apache.org/docs/latest/ml-classification-regression.html#naive-bayes) classifier to our problem. \n",
    "Take the time to read the doc and apply it to our problem. You will first have to associate a number to each label. You can use the [`String Indexer`](https://spark.apache.org/docs/2.1.0/ml-features.html#stringindexer) function of pyspark for that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation of the label into a number\n",
    "# Instanciate\n",
    "indexer = StringIndexer(inputCol=\"_c0\", outputCol=\"label\")\n",
    "# Train transform\n",
    "model = indexer.fit(train_trans)\n",
    "indexed_train = model.transform(train_trans)\n",
    "indexed_test = model.transform(test_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "nb = NaiveBayes(smoothing=1)\n",
    "model = nb.fit(indexed_train)\n",
    "predictions = model.transform(indexed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model : 0.625527911891\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print('Accuracy of the model : ' + str(evaluator.evaluate(predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Evaluation of the learned model can be done by using the [evaluation](https://spark.apache.org/docs/2.1.0/mllib-evaluation-metrics.html) module of MLlib. Print the accuracy of the obtained model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
